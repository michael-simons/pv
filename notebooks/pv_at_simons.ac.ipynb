{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import suncalc\n",
    "\n",
    "tz='Europe/Berlin'\n",
    "\n",
    "# Create a connection to your PV database. \n",
    "# Make sure you installed the schema as described in the README of the pv project.\n",
    "db = duckdb.connect('pv.db')\n",
    "\n",
    "# A snapshot of my databases content can also be retrieved with DuckDB running in-memory in this very notebook itself.\n",
    "# It works by installing the httpfs extension and than running a database import\n",
    "# db = duckdb.connect(database=':memory:')\n",
    "# db.sql('INSTALL httpfs')\n",
    "# db.sql('LOAD httpfs')\n",
    "# db.sql(\"IMPORT database 'http://simons.ac/pv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc275580",
   "metadata": {},
   "source": [
    "# Photovoltaics at simons.ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e5c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(f\"\"\"\n",
    "Last update: _{datetime.now().strftime('%c')}_.\n",
    "\n",
    "The sources, including the SQL queries and solar course computations are available as a Jupyter notebook in my pv repository: [michael-simons/pv](https://github.com/michael-simons/pv).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac2518",
   "metadata": {},
   "source": [
    "## Technical background\n",
    "\n",
    "This is a quick overview over our photovoltaics installation in Aachen. We started planning this in September 2022. Originally, I didn't want to put too much thought and effort into analyzing the result, but it turned out: The smart devices vendors sell you are still not that smart, even in 2023. So here I am, collecting measurements straight from our SunSpec compatible inverter over Modbus and dumping it straight into a [DuckDB](https://duckdb.org) analytical database. The [logger](https://github.com/michael-simons/pv/tree/main/logger) I created for that purpose is based on the [Energy systems reading toolkit](https://energy.basjes.nl) by [Niels Basjes](https://mastodon.basjes.nl/@niels). \n",
    "\n",
    "The database design follows a subset of the [pink database design](https://www.salvis.com/blog/2018/07/18/the-pink-database-paradigm-pinkdb/#feature_3). Instead of writing down all the analytic queries straight into the notebook or my programming environment of choice, I created a bunch of views that basically represents the api of my scheme. Please visit the above lined repository for more information.\n",
    "\n",
    "## General reasons\n",
    "\n",
    "As both gas and electricy prices have been erratic the last years, even more so since the Russian invasion into Ukraine since February 2022 and these days with the increasing inflation, we wanted to invest into a somewhat more stable price in the long run. As reference, I added the current prices we have for buying a kWh electric energy into the database. The price has seen a whooping increase of more than 75% since 2010:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "buying_prices = db.sql(\"SELECT valid_from AS year, net FROM buying_prices\").df()\n",
    "imported = db.sql(\"SELECT date_trunc('year', period_end) as year, import FROM official_measurements ORDER by period_start ASC\").df()\n",
    "\n",
    "imported_with_price = pd.merge_asof(buying_prices, imported, on=\"year\")\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "imported_with_price.plot(x = 'year', y = 'net',    ylabel='Net price in ct/kWh', ax = ax)\n",
    "imported_with_price.plot(x = 'year', y = 'import', ylabel='Imported energy in kWh', ax = ax, secondary_y = True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6001c3",
   "metadata": {},
   "source": [
    "The amount of energy we use has been relatively constant over the years. There's a slight increase with me working remotely since 2018 and the kids playing more and more video games these days, but that's neglectable. \n",
    "\n",
    "## The setup\n",
    "\n",
    "We went for an installation of 10.53kWp, in a split east/west setup. In total we have 26 [Solarwatt Glass-Glass \"Panel Vision AM 4.0\"](https://solarwatt.canto.global/direct/document/2bp9ip7a492p51429ek8qdr706/HPyRw7XsY0A3hu2B1c6SpNGxiOc/original?content-type=application%2Fpdf&name=Datenblatt+SOLARWATT+Panel+vision+AM+4.0+pure+de.pdf) modules with an output of 405Wp each, in plus selection:\n",
    "\n",
    "* 14 directed east\n",
    "* 12 directed west\n",
    "\n",
    "The inverter is a Kaco [blueplanet 10.0 NX3 M2](https://kaco-newenergy.com/de/produkte/blueplanet-3.0-20.0-NX3-M2/). The possible peak output ov 10.53kWp is a lot more than we would actually use in a year, but we wanted to be prepared for the future. We haven't yet decided to replace our 10+ year old car with an electric one or if we go straight for a stationary battery solution followed by replacing the heating system with a heatpump eventually. Time will tell. Until then, let's have a look at the current results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c57d83e",
   "metadata": {},
   "source": [
    "## The results\n",
    "\n",
    "### Energy production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10147c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "period = db.sql('SELECT min(measured_on) AS min, max(measured_on) AS max FROM measurements').df()\n",
    "Markdown(f\"\"\"\n",
    "The measurement in the currently loaded dataset spans right now from **{period['min'][0].strftime('%Y-%m-%d')}** to **{period['max'][0].strftime('%Y-%m-%d')}** and in that period we had the following overall production:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ce651",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.sql('SELECT * FROM overall_production').df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed6c63",
   "metadata": {},
   "source": [
    "with averages per month as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486fc79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_per_month = db.sql('SELECT * FROM average_production_per_month').df()[['Month', 'kWh']].dropna()\n",
    "average_per_month.plot(kind='bar', x='Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea949b2",
   "metadata": {},
   "source": [
    "As of writing, we still haven't had any day really close to possible peak. However, our original assumption that we are better of using both side of the house holds true. The inverter boots up pretty close to sunrise and already in May we had several evenings where the whole setup was producing enough energy for late cooking. Plotting the production per hour shows that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222772dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_per_hour = db.sql('SELECT * FROM average_production_per_hour').df()[['Hour', 'kWh']].dropna()\n",
    "average_per_hour.plot(kind='barh', x='Hour').invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812efafc",
   "metadata": {},
   "source": [
    "The production for the best performing day so far looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd596f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_day = db.sql('SELECT * FROM best_performing_day').df()\n",
    "lat_long = db.sql('SELECT * FROM place_of_installation').df()\n",
    "lat, long = lat_long['lat'][0], lat_long['long'][0]\n",
    "\n",
    "localize_tz = lambda v: v.tz_localize(tz=tz)\n",
    "to_degree = lambda v: math.degrees(v) if v > 0 else 0\n",
    "\n",
    "best_day['measured_on'] = best_day['measured_on'].apply(localize_tz)\n",
    "best_day['altitude'] = pd.DataFrame(suncalc.get_position(best_day['measured_on'], long, lat)['altitude'].apply(to_degree))\n",
    "\n",
    "_, ax = plt.subplots(figsize=plt.figaspect(1/2))\n",
    "best_day.plot(x = 'measured_on', y='production', ylabel='W', ax = ax)\n",
    "best_day.plot(x = 'measured_on', y='altitude', ylabel='Sun altitude in Â°', ax = ax, secondary_y = True,  mark_right=False)\n",
    "\n",
    "sun_times = suncalc.get_times(best_day['measured_on'][0].floor('d') + timedelta(days=1), long, lat)\n",
    "sun_times = {k: v.tz_localize(tz='UTC').tz_convert(tz='Europe/Berlin') for k, v in sun_times.items()}\n",
    "\n",
    "ax.axvspan(sun_times['dawn'], sun_times['sunrise_end'], color='red', alpha=0.1)\n",
    "ax.axvspan(sun_times['solar_noon'] - timedelta(minutes=5), sun_times['solar_noon'] + timedelta(minutes=5), color='yellow', alpha=0.4)\n",
    "ax.axvspan(sun_times['sunset_start'], sun_times['dusk'], color='blue', alpha=0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c49b7",
   "metadata": {},
   "source": [
    "The orange chart is the sun position at our location, the red bar the time from dawn until sunrise end, the yellow bar a 10 minute period around true or solar noon, the blue bar finally is the time period from sunset start until dusk.\n",
    "\n",
    "I personally liked the visualization that [Oli](https://social.tchncs.de/@oli) did at [Ein Jahr Photovoltaik: \"Tolle Dinge, die man mit SQL machen kann\"](https://tonick.net/p/2022/12/ein-jahr-photovoltaik/) so much, that I wanted to have this too. Turns out, it is easy enough with DuckDB 0.8.0 and its excellent support for the `PIVOT` statement (See the corresponding view [`average_production_per_month_and_hour`](https://github.com/michael-simons/pv/blob/main/sql/schema/R__Create_view_average_production_per_month_and_hour.sql). Together with the \"Styler\" feature of Pandas and the integraded color maps of matplotlib you get a colorized table based on the value of the cells with zero effort. In the below example I drop the original dataframe index column and use my `Hour` column instead. It is on purpose that it doesn't get a color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db.sql('SELECT * FROM average_production_per_month_and_hour').df().set_index([\"Hour\"]).fillna(0)\n",
    "df.style.background_gradient(cmap=\"YlOrRd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a95a0d",
   "metadata": {},
   "source": [
    "A sun course diagram drawn for the location of our house shows that we won't have much output in the winter times, basically some sune from between 9:00 in the morning until 17:00 in the afternoon, at an angle that is probably not very productive. The below diagram has 4 plots: The course of the sun at winter and summer solstice plus the most productive day in the same colors as above and March 21st in grey as a reference value in between the two extremes. The fancy things shaped like eights are called [Analemma](https://en.wikipedia.org/wiki/Analemma); a figure that is the result of observing the variying course of the sun from a fixed location on earth at the same solar time over the year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd1b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.today()\n",
    "dt = datetime(dt.year, 1, 1)\n",
    "\n",
    "times = pd.date_range(datetime(dt.year, 1, 1), datetime(dt.year+1, 1, 1), inclusive='left', freq='H', tz=tz)\n",
    "solpos = pd.DataFrame(suncalc.get_position(times, long, lat))\n",
    "solpos = pd.DataFrame(times).join(solpos, how='inner').loc[solpos['altitude'] > 0, :]\n",
    "solpos = solpos.set_index(0)\n",
    "solpos['altitude'] = np.vectorize(to_degree)(solpos['altitude'])\n",
    "solpos['azimuth'] = np.vectorize(lambda v: math.degrees(v))(solpos['azimuth'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plt.figaspect(1/3))\n",
    "points = ax.scatter(solpos['azimuth'], solpos['altitude'], s=2, c=solpos.index.dayofyear, label=None)\n",
    "fig.colorbar(points)\n",
    "\n",
    "for hour in np.unique(solpos.index.hour):\n",
    "    subset = solpos.loc[solpos.index.hour == hour, :]\n",
    "    height = subset['altitude']\n",
    "    pos = solpos.loc[height.idxmax(), :]\n",
    "    ax.text(pos['azimuth'], pos['altitude'], str(hour))\n",
    "    \n",
    "cmap = plt.colormaps.get_cmap('YlOrRd')\n",
    "neutral_day = datetime(dt.year, 3, 21)\n",
    "dates_of_interest = [datetime(dt.year-1, 12, 21), best_day['measured_on'][0].tz_localize(None).to_pydatetime(), neutral_day, datetime(dt.year, 6, 21)]\n",
    "dates_of_interest.sort()\n",
    "dates_of_interest[0] = dates_of_interest[0].replace(year=dt.year)\n",
    "\n",
    "for index, date in enumerate(pd.to_datetime(dates_of_interest)):\n",
    "    times = pd.date_range(date, date+pd.Timedelta('24h'), freq='1min', tz=tz)\n",
    "    solpos = pd.DataFrame(suncalc.get_position(times, long, lat))\n",
    "    solpos = solpos.loc[solpos['altitude'] > 0, :]\n",
    "    solpos['altitude'] = np.vectorize(to_degree)(solpos['altitude'])\n",
    "    solpos['azimuth'] = np.vectorize(lambda v: math.degrees(v))(solpos['azimuth'])\n",
    "    label = date.strftime('%Y-%m-%d')\n",
    "    color = 'grey' if date == neutral_day else cmap(0.4 + 0.2 * index)\n",
    "    ax.plot(solpos['azimuth'], solpos['altitude'], label=label, color=color)\n",
    "    \n",
    "fig.legend(loc='center right')\n",
    "ax.set_xlabel('Solar Azimuth (degrees, measured from south to west)')\n",
    "ax.set_ylabel('Solar Elevation (degrees)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037965e5",
   "metadata": {},
   "source": [
    "### Money and amortization\n",
    "\n",
    "Sadly, we don't have full import / export data right from the start (did I mention smart devices suck?), so for the first month I have no real compensation for feeding back into the grid. Therefor - and for comparison - I have fictitious curve in the following amortization chart assuming a full export into the grid and starting a bit later, the actual curve adding the compensation we got plus the money we saved for not having to import energy.\n",
    "\n",
    "The compensation for an actual full export to the grid is slighly higher than for a partial one. However as of writing, I buy nearly for tripple the amount than I could sell at. Kinda odd if you ask me.\n",
    "\n",
    "#### Actuall amortization\n",
    "\n",
    "Just weighting the net cost of acquisation against the a fictitious full export of the produced energy plus our actual compensation for feeding in plus the saved money not buying energy. I should probably not look too long at that chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "amortization = db.sql(\"SELECT month, fictitious_full_export FROM amortization\").df()\n",
    "amortization = amortization.set_index([\"month\"])\n",
    "amortization.plot(figsize=plt.figaspect(1/2))\n",
    "plt.ylim(top=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
